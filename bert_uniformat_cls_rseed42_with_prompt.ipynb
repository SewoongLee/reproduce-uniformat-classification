{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of the UniformatBridge model implementation\n",
    "\n",
    "- This is an **_unofficial_** reproduction of the experiment described in \"Transformer language model for mapping construction schedule activities to uniformat categories\" by Yoonhwa Jung, Julia Hockenmaier, and Mani Golparvar-Fard, 2024.\n",
    "- The study can be accessed at https://doi.org/10.1016/j.autcon.2023.105183."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.6\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6p: \"Five epochs with a batch size of 32, a dropout rate of 0.1, and a learning rate of 1.5e−5 were used to fine-tune the model.\"\n",
    "epoch_size = 1#5\n",
    "batch_size = 32\n",
    "dropout_rate = 0.1  # BertForSequenceClassification default dropout_rate = 0.1\n",
    "learning_rate = 1.5e-5\n",
    "\n",
    "rseed = 42  # 7p: \"In Table 2 and 3, 𝜇 is the average performance on three random seeds, and 𝜎 is their standard deviation.\"\n",
    "dataset_path = \"E:/_datasets/jung_et_al_2024/\"  # Data should not be shared publicly.\n",
    "experiment_name = 'with_prompt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIM and ASTM Uniformat categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(dataset_path+\"0-overall-level3.xlsx\", header=0)\n",
    "\n",
    "cls = 'Level3'\n",
    "df = df.loc[:, ['predwbs2', 'predwbs', 'predtask', 'wbs2', 'wbs', 'name', 'sucwbs2', 'sucwbs', 'suctask', cls]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  CONSTRUCTION SUPERSTRUCTURE > Roof > Set Mechanical Equipment [pred] CONSTRUCTION SUPERSTRUCTURE > Roof > Pipe Mechanical Equipment [succ]  100 KINGSHIGHWAY > COMMISSIONNG & INSPECTIONS > STARTUP\n",
      "label:  D2040\n"
     ]
    }
   ],
   "source": [
    "new_tokens = ['[pred]', '[succ]']\n",
    "\n",
    "df['text'] = df.apply(\n",
    "    lambda row: f\"{row['predwbs2']} > {row['predwbs']} > {row['predtask']} [pred] {row['wbs2']} > {row['wbs']} > {row['name']} [succ] {row['sucwbs2']} > {row['sucwbs']} > {row['suctask']}\",\n",
    "    axis=1\n",
    ")\n",
    "df['label'] = df[cls]\n",
    "\n",
    "print_idx = 6538  # D2040: 'Rain Water Drainage' case in Table 4\n",
    "print('text: ', df['text'][print_idx])\n",
    "print('label: ', df['label'][print_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20469, 6823, 6824)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = df['text']\n",
    "labels = df['label']\n",
    "\n",
    "# 6p: \"This dataset is further split into training, validation, and testing using a 60-20-20 distribution.\"\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.4, random_state=rseed)\n",
    "validation_texts, test_texts, validation_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=rseed)\n",
    "\n",
    "(len(train_texts), len(validation_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "train_labels_encoded = encoded_labels[train_texts.index]\n",
    "validation_labels_encoded = encoded_labels[validation_texts.index]\n",
    "test_labels_encoded = encoded_labels[test_texts.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t30522: AddedToken(\"[pred]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " \t30523: AddedToken(\"[succ]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       " },\n",
       " BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30524, 768)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=50, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(set(encoded_labels))).to(device)\n",
    "\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def make_dataloader(plain_texts, encoded_labels, batch_size=32):\n",
    "    '''\n",
    "    plain_texts: list of strings (should be encoded when making dataloader due to tokenizer padding/truncation)\n",
    "    encoded_labels: list of class indices (should be encoded beforehand using LabelEncoder with the entire dataset)\n",
    "    '''\n",
    "    texts = tokenizer(plain_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "    labels = torch.tensor(encoded_labels, dtype=torch.long).to(device)\n",
    "    dataset = TensorDataset(\n",
    "        texts['input_ids'], \n",
    "        texts['attention_mask'], \n",
    "        labels,\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "train_loader = make_dataloader(train_texts.tolist(), train_labels_encoded, batch_size)\n",
    "validation_loader = make_dataloader(validation_texts.tolist(), validation_labels_encoded, batch_size)\n",
    "test_loader = make_dataloader(test_texts.tolist(), test_labels_encoded, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ★ Additional Datasets: ASTM Uniformat II Classification for Building Elements Description\n",
    "- https://www.govinfo.gov/content/pkg/GOVPUB-C13-5af96252bc88826c911daac93c449927/pdf/GOVPUB-C13-5af96252bc88826c911daac93c449927.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Publicly available definition of ASTM Uniformat II + CoT Prompt\n",
    "# df_astm = pd.read_csv(\"public_astm_uniformat_ii_classification.csv\", header=0)\n",
    "\n",
    "# df_astm = df_astm[df_astm['Class'].isin(label_encoder.classes_)] # Use only the classes that are in the dataset\n",
    "# df_astm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ★ Additional Datasets: Data augmentation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# def generate_with_gpt2(prompt_text, max_len=100, repetition_penalty=1.2):\n",
    "#     inputs = gpt2_tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "#     outputs = gpt2_model.generate(\n",
    "#         inputs, \n",
    "#         pad_token_id=gpt2_tokenizer.eos_token_id, \n",
    "#         max_length=max_len, \n",
    "#         # do_sample=True, temperature=0.9, # Probabilistic\n",
    "#         repetition_penalty=repetition_penalty # Deterministic\n",
    "#     )\n",
    "#     generated_text = gpt2_tokenizer.decode(outputs[0])\n",
    "\n",
    "#     return generated_text\n",
    "\n",
    "# generate_with_gpt2(\"Once upon a time,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Intuition\n",
    "# generate_with_gpt2(\"Examples of building structure components for services of plumbing and rain water drainage are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_with_gpt2_for_astm(level1, level2, level3):\n",
    "#     prompt = f\"Examples of building structure components for {level1.lower()} of {level2.lower()} and {level3.lower()} are\"\n",
    "#     return level1 + \" \" + level2 + \" \" + level3 + \" \" + repr(generate_with_gpt2(prompt)[len(prompt):])\n",
    "\n",
    "# generate_with_gpt2_for_astm(\"SERVICES\", \"Plumbing\", \"Rain Water Drainage\")  # D2040 in ASTM Uniformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # astm_texts = df_astm.apply(\n",
    "# #     lambda row: f\"[pred] {generate_with_gpt2_for_astm(row['Level1'], row['Level2'], row['Level3'])} [succ]\",\n",
    "# #     axis=1\n",
    "# # )\n",
    "\n",
    "\n",
    "# # # Test # TODO: delete\n",
    "# astm_texts = pd.concat([\n",
    "#     pd.Series(100*[\"CONSTRUCTION SUPERSTRUCTURE > Roof > Set Mechanical Equipment [pred] CONSTRUCTION SUPERSTRUCTURE > Roof > Pipe Mechanical Equipment [succ]  100 KINGSHIGHWAY > COMMISSIONNG & INSPECTIONS > STARTUP\"]),\n",
    "#     pd.Series(100*[\"Garage Garage Structure | MEP FP | Finishes > LL1 > OH Sprinkler Piping Rough In LL1 [pred] Garage Garage Structure | MEP FP | Finishes > LL1 > OH Storm Drainage Piping Rough In LL1 [succ] Garage Garage Structure | MEP FP | Finishes > LL1 > Install Pipe Guards | Bollards LL1\"]),\n",
    "# ])\n",
    "# astm_labels = pd.Series([\"[D2040]\"]*200)\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(astm_texts[i], '...', astm_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ★ New Fine-tuning DataLoader with ASTM Uniformat Class Definition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# astm_loader = make_dataloader(astm_texts.tolist(), label_encoder.transform(astm_labels.tolist()), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning (Train & Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 640/640 [02:11<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.6296933436766268 | Validation Loss: 0.48467279128103613\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epoch_size):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for input_ids, attention_mask, labels in tqdm(train_loader):\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for input_ids, attention_mask, labels in validation_loader:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    val_losses.append(val_loss / len(validation_loader))\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_losses[-1]} | Validation Loss: {val_losses[-1]}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    df_test_rows = []\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred_labels.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        for b in range(input_ids.shape[0]):\n",
    "            text = tokenizer.decode([token for i, token in enumerate(input_ids[b]) if attention_mask[b][i] == 1])\n",
    "            pred_label = label_encoder.classes_[torch.argmax(outputs.logits[b]).item()]\n",
    "            true_label = label_encoder.classes_[labels[b].item()]\n",
    "            df_test_rows.append({'text': text, 'true_label': true_label, 'pred_label': pred_label, 'correct': pred_label == true_label})\n",
    "\n",
    "# Data should not be shared publicly.\n",
    "pd.DataFrame(df_test_rows).to_csv(dataset_path+f\"/test_results_{experiment_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9333\n",
      "Precision: 0.6329\n",
      "Recall: 0.5996\n",
      "F1 Score: 0.6023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    true_labels, \n",
    "    pred_labels, \n",
    "    # average='weighted', \n",
    "    average='macro',  # Original paper setting (page 8)\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-wise Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1010</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1020</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.873016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1030</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.757576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2010</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.788321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B1010</td>\n",
       "      <td>0.968147</td>\n",
       "      <td>0.996028</td>\n",
       "      <td>0.981889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B1020</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.931298</td>\n",
       "      <td>0.779553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B2010</td>\n",
       "      <td>0.982175</td>\n",
       "      <td>0.996383</td>\n",
       "      <td>0.989228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B2020</td>\n",
       "      <td>0.990881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B2030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B3010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B3020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C1010</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.998117</td>\n",
       "      <td>0.951526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C1020</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C1030</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.854701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C2010</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.682540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C2020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C3010</td>\n",
       "      <td>0.992151</td>\n",
       "      <td>0.984424</td>\n",
       "      <td>0.988272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C3020</td>\n",
       "      <td>0.970093</td>\n",
       "      <td>0.990458</td>\n",
       "      <td>0.980170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C3030</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.985836</td>\n",
       "      <td>0.984441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D1010</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D1020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D1090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D2010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D2020</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.969432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D2030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D2040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D2090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D3010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D3020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>D3030</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>D3040</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.963563</td>\n",
       "      <td>0.940711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>D3050</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.974790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>D3060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>D3070</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>D4010</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>D4020</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.985075</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>D4030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>D4090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>D5010</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.884956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>D5020</td>\n",
       "      <td>0.940141</td>\n",
       "      <td>0.974453</td>\n",
       "      <td>0.956989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>D5030</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.926641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>D5090</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.663507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>E1010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>E1020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>E1030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>E1090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>E2010</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.879518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>E2020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class  Precision    Recall  F1 Score\n",
       "0   A1010   0.722892  0.895522  0.800000\n",
       "1   A1020   0.846154  0.901639  0.873016\n",
       "2   A1030   0.819672  0.704225  0.757576\n",
       "3   A2010   0.857143  0.729730  0.788321\n",
       "4   A2020   0.000000  0.000000  0.000000\n",
       "5   B1010   0.968147  0.996028  0.981889\n",
       "6   B1020   0.670330  0.931298  0.779553\n",
       "7   B2010   0.982175  0.996383  0.989228\n",
       "8   B2020   0.990881  1.000000  0.995420\n",
       "9   B2030   0.000000  0.000000  0.000000\n",
       "10  B3010   1.000000  0.147541  0.257143\n",
       "11  B3020   0.000000  0.000000  0.000000\n",
       "12  C1010   0.909091  0.998117  0.951526\n",
       "13  C1020   0.916031  1.000000  0.956175\n",
       "14  C1030   0.806452  0.909091  0.854701\n",
       "15  C2010   0.977273  0.524390  0.682540\n",
       "16  C2020   0.000000  0.000000  0.000000\n",
       "17  C3010   0.992151  0.984424  0.988272\n",
       "18  C3020   0.970093  0.990458  0.980170\n",
       "19  C3030   0.983051  0.985836  0.984441\n",
       "20  D1010   0.924242  1.000000  0.960630\n",
       "21  D1020   0.000000  0.000000  0.000000\n",
       "22  D1090   1.000000  0.500000  0.666667\n",
       "23  D2010   1.000000  0.545455  0.705882\n",
       "24  D2020   0.956897  0.982301  0.969432\n",
       "25  D2030   1.000000  0.823529  0.903226\n",
       "26  D2040   0.000000  0.000000  0.000000\n",
       "27  D2090   0.000000  0.000000  0.000000\n",
       "28  D3010   0.000000  0.000000  0.000000\n",
       "29  D3020   0.000000  0.000000  0.000000\n",
       "30  D3030   0.721649  1.000000  0.838323\n",
       "31  D3040   0.918919  0.963563  0.940711\n",
       "32  D3050   0.991453  0.958678  0.974790\n",
       "33  D3060   0.000000  0.000000  0.000000\n",
       "34  D3070   0.977273  0.977273  0.977273\n",
       "35  D4010   0.972222  1.000000  0.985915\n",
       "36  D4020   0.970588  0.985075  0.977778\n",
       "37  D4030   1.000000  0.857143  0.923077\n",
       "38  D4090   0.000000  0.000000  0.000000\n",
       "39  D5010   0.806452  0.980392  0.884956\n",
       "40  D5020   0.940141  0.974453  0.956989\n",
       "41  D5030   0.937500  0.916031  0.926641\n",
       "42  D5090   0.614035  0.721649  0.663507\n",
       "43  E1010   0.000000  0.000000  0.000000\n",
       "44  E1020   0.000000  0.000000  0.000000\n",
       "45  E1030   0.000000  0.000000  0.000000\n",
       "46  E1090   1.000000  0.611111  0.758621\n",
       "47  E2010   0.869048  0.890244  0.879518\n",
       "48  E2020   0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average=None, zero_division=0)\n",
    "\n",
    "class_names = label_encoder.inverse_transform(list(set(true_labels)))\n",
    "\n",
    "performance_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1\n",
    "})\n",
    "performance_df.to_csv(f'experimental_results_rseed{rseed}_{experiment_name}.csv')\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
